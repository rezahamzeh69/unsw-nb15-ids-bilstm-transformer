{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcKnUjUVz14lCZsAhfCUw2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezahamzeh69/unsw-nb15-ids-bilstm-transformer/blob/main/robust_transformer_bilstm_ids.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03-xATAH0WUS",
        "outputId": "b55beeb0-5e96-4d43-e896-e1305d6d0051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading UNSW-NB15 dataset using kagglehub...\n",
            "Using Colab cache for faster access to the 'unswnb15' dataset.\n",
            "Dataset downloaded to: /kaggle/input/unswnb15\n",
            "Attempting to load training data from: /kaggle/input/unswnb15/UNSW_NB15_training-set.parquet\n",
            "Attempting to load testing data from: /kaggle/input/unswnb15/UNSW_NB15_testing-set.parquet\n",
            "Datasets loaded successfully.\n",
            "Preprocessing datasets...\n",
            "Preprocessing finished. Input dimension: 188\n",
            "Creating sequences...\n",
            "Raw training sequences: 175327\n",
            "Raw testing sequences: 82318\n",
            "Splitting 15.0% of training sequences for validation.\n",
            "Final Train sequences: 149027\n",
            "Validation sequences: 26300\n",
            "Final Test sequences: 82318\n",
            "Using normalized class weights for CrossEntropyLoss: [1.5658044 0.7345646]\n",
            "[Resume] Loaded checkpoint from './unsw_bilstm_trf_adv_ig_checkpoints/ckpt_latest.pt' at epoch 24\n",
            "\n",
            "--- Starting Training on cuda ---\n",
            "Epoch 25/30 | Train Loss: 0.0124 | Val Loss: 0.0609 | Val Acc: 0.9846 | Val F1: 0.9887 | Time: 87.28s\n",
            "Early stopping triggered after epoch 25!\n",
            "--- Training Finished --- Total time: 1m 27s\n",
            "Loaded best model weights from './unsw_bilstm_trf_adv_ig_checkpoints/ckpt_best.pt' for final evaluation.\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "Test Loss: 0.0545\n",
            "Test Accuracy: 0.9857\n",
            "Test F1 Score (Binary): 0.9871\n",
            "\n",
            "Confusion Matrix:\n",
            "[[36304   682]\n",
            " [  492 44840]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Normal (0)       0.99      0.98      0.98     36986\n",
            "  Attack (1)       0.99      0.99      0.99     45332\n",
            "\n",
            "    accuracy                           0.99     82318\n",
            "   macro avg       0.99      0.99      0.99     82318\n",
            "weighted avg       0.99      0.99      0.99     82318\n",
            "\n",
            "--------------------------------\n",
            "[RobustEval] Clean Acc: 0.9857 | Adv Acc (pgd, eps=0.01): 0.9847\n",
            "[RobustEval] Clean Acc: 0.9857 | Adv Acc (pgd, eps=0.03): 0.9826\n",
            "Saved attention weights to ./unsw_bilstm_trf_adv_ig_checkpoints/attn_weights.npy (shape: H x T x T).\n",
            "Saved Integrated Gradients attributions to ./unsw_bilstm_trf_adv_ig_checkpoints/ig_attributions.npy (shape: B x T x D).\n",
            "Saved IG time and feature importance.\n",
            "Saved IG feature importance CSV to ./unsw_bilstm_trf_adv_ig_checkpoints/ig_feature_importance.csv.\n",
            "\n",
            "Execution finished successfully.\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "import io\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from contextlib import contextmanager\n",
        "import kagglehub\n",
        "\n",
        "# =========================\n",
        "# Hyperparameters\n",
        "# =========================\n",
        "LSTM_HIDDEN_DIM = 128\n",
        "LSTM_LAYERS = 2\n",
        "TRANSFORMER_DIM = 128\n",
        "NHEAD = 8\n",
        "NUM_TRANSFORMER_LAYERS = 2\n",
        "NUM_CLASSES = 2\n",
        "DROPOUT = 0.3\n",
        "SEQUENCE_LENGTH = 15\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_EPOCHS = 30\n",
        "VALIDATION_SPLIT_FROM_TRAIN = 0.15\n",
        "RANDOM_SEED = 42\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Adversarial Training\n",
        "ADV_TRAINING = True          # True/False\n",
        "ADV_METHOD = 'pgd'           # 'fgsm' or 'pgd'\n",
        "ADV_EPS = 0.03\n",
        "ADV_ALPHA = 0.007\n",
        "ADV_STEPS = 7\n",
        "ADV_LAMBDA = 0.5\n",
        "\n",
        "# Interpretability\n",
        "IG_STEPS = 32                # تعداد گام‌های مسیر IG\n",
        "IG_BASELINE = 'zeros'        # 'zeros' or 'random'\n",
        "SAVE_INTERPRETABILITY = True # ذخیره خروجی‌ها روی دیسک\n",
        "\n",
        "# Checkpoint/Resume\n",
        "AUTO_RESUME = True           # اگر True باشد، در شروع، به‌صورت خودکار از آخرین چک‌پوینت ادامه می‌دهد\n",
        "PROJECT_NAME = \"unsw_bilstm_trf_adv_ig\"\n",
        "def _default_ckpt_dir():\n",
        "    # اگر Google Drive mount شده باشد، درایو را ترجیح بده\n",
        "    if os.path.exists(\"/content/drive/MyDrive\"):\n",
        "        return f\"/content/drive/MyDrive/{PROJECT_NAME}_checkpoints\"\n",
        "    # در Kaggle/Colab بدون درایو:\n",
        "    if os.path.exists(\"/kaggle/working\"):\n",
        "        return f\"/kaggle/working/{PROJECT_NAME}_checkpoints\"\n",
        "    return f\"./{PROJECT_NAME}_checkpoints\"\n",
        "\n",
        "CKPT_DIR = _default_ckpt_dir()\n",
        "CKPT_LATEST = os.path.join(CKPT_DIR, \"ckpt_latest.pt\")\n",
        "CKPT_BEST = os.path.join(CKPT_DIR, \"ckpt_best.pt\")\n",
        "\n",
        "TRAIN_FILE_NAME = 'UNSW_NB15_training-set.parquet'\n",
        "TEST_FILE_NAME = 'UNSW_NB15_testing-set.parquet'\n",
        "\n",
        "# Globals for interpretability/masking\n",
        "FEATURE_NAMES = None              # لیست ستون‌های ویژگی بعد از وان‌هات\n",
        "PERTURB_MASK = None              # ماسک ویژگی‌های قابل اغتشاش (1=عددی، 0=وان‌هات دسته‌ای)\n",
        "\n",
        "# =========================\n",
        "# Reproducibility\n",
        "# =========================\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# =========================\n",
        "# Utils: Checkpointing\n",
        "# =========================\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(state, is_best=False):\n",
        "    ensure_dir(CKPT_DIR)\n",
        "    torch.save(state, CKPT_LATEST)\n",
        "    if is_best:\n",
        "        shutil.copyfile(CKPT_LATEST, CKPT_BEST)\n",
        "\n",
        "def try_load_checkpoint(model, optimizer):\n",
        "    \"\"\"\n",
        "    اگر چک‌پوینت موجود باشد، مدل/اپتیمایزر/ایپاک/هیستوری/early stopping را برمی‌گرداند.\n",
        "    \"\"\"\n",
        "    if AUTO_RESUME and os.path.exists(CKPT_LATEST):\n",
        "        ckpt = torch.load(CKPT_LATEST, map_location=DEVICE)\n",
        "        model.load_state_dict(ckpt['model_state'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer_state'])\n",
        "        start_epoch = ckpt.get('epoch', 0) + 1\n",
        "        best_val_loss = ckpt.get('best_val_loss', float('inf'))\n",
        "        epochs_no_improve = ckpt.get('epochs_no_improve', 0)\n",
        "        history = ckpt.get('history', {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1_score': []})\n",
        "        print(f\"[Resume] Loaded checkpoint from '{CKPT_LATEST}' at epoch {start_epoch}\")\n",
        "        return start_epoch, best_val_loss, epochs_no_improve, history\n",
        "    return 0, float('inf'), 0, {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1_score': []}\n",
        "\n",
        "# =========================\n",
        "# Data Loading & Preprocess\n",
        "# =========================\n",
        "def load_and_preprocess_unsw(dataset_dir):\n",
        "    global FEATURE_NAMES, PERTURB_MASK\n",
        "    train_file_path = os.path.join(dataset_dir, TRAIN_FILE_NAME)\n",
        "    test_file_path = os.path.join(dataset_dir, TEST_FILE_NAME)\n",
        "    print(f\"Attempting to load training data from: {train_file_path}\")\n",
        "    print(f\"Attempting to load testing data from: {test_file_path}\")\n",
        "\n",
        "    if not os.path.exists(train_file_path) or not os.path.exists(test_file_path):\n",
        "        print(\"Error: Training or Testing file not found in the downloaded dataset directory.\")\n",
        "        print(f\"Contents of {dataset_dir}: {os.listdir(dataset_dir)}\")\n",
        "        return None, None, None, None, -1\n",
        "\n",
        "    try:\n",
        "        df_train = pd.read_parquet(train_file_path)\n",
        "        df_test = pd.read_parquet(test_file_path)\n",
        "        print(\"Datasets loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Parquet files: {e}\")\n",
        "        return None, None, None, None, -1\n",
        "\n",
        "    print(\"Preprocessing datasets...\")\n",
        "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "    df = df.drop(['id', 'attack_cat'], axis=1, errors='ignore')\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace(r'[^a-z0-9_]', '', regex=True)\n",
        "\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    for col in numerical_cols:\n",
        "        if df[col].isnull().any():\n",
        "            df[col] = df[col].fillna(df[col].median() if df[col].nunique() > 10 else 0)\n",
        "    for col in categorical_cols:\n",
        "        if df[col].isnull().any():\n",
        "            if pd.api.types.is_categorical_dtype(df[col]):\n",
        "                df[col] = df[col].astype('object')\n",
        "            df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "    if 'label' in numerical_cols:\n",
        "        numerical_cols.remove('label')\n",
        "    elif 'label' in categorical_cols:\n",
        "        categorical_cols.remove('label')\n",
        "        try:\n",
        "            df['label'] = pd.to_numeric(df['label'])\n",
        "        except ValueError:\n",
        "            print(\"Error: Could not convert 'label' column to numeric.\")\n",
        "            return None, None, None, None, -1\n",
        "\n",
        "    if 'label' not in df.columns or not pd.api.types.is_numeric_dtype(df['label']):\n",
        "        print(\"Error: 'label' column is missing or not numeric after cleaning.\")\n",
        "        return None, None, None, None, -1\n",
        "\n",
        "    # One-hot\n",
        "    df_encoded = pd.get_dummies(df, columns=categorical_cols, dummy_na=False)\n",
        "\n",
        "    train_len = len(df_train)\n",
        "    df_train_processed = df_encoded.iloc[:train_len].copy()\n",
        "    df_test_processed = df_encoded.iloc[train_len:].copy()\n",
        "\n",
        "    # Scaling only numeric parts (after encoding)\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical_cols_encoded = df_train_processed.drop('label', axis=1).select_dtypes(include=np.number).columns.tolist()\n",
        "    if numerical_cols_encoded:\n",
        "        for col in numerical_cols_encoded:\n",
        "            df_train_processed[col] = df_train_processed[col].astype(np.float32)\n",
        "            df_test_processed[col] = df_test_processed[col].astype(np.float32)\n",
        "        scaler.fit(df_train_processed[numerical_cols_encoded])\n",
        "        df_train_processed.loc[:, numerical_cols_encoded] = scaler.transform(df_train_processed[numerical_cols_encoded])\n",
        "        df_test_processed.loc[:, numerical_cols_encoded] = scaler.transform(df_test_processed[numerical_cols_encoded])\n",
        "\n",
        "    feature_cols = df_train_processed.columns.drop('label')\n",
        "    FEATURE_NAMES = list(feature_cols)\n",
        "\n",
        "    # ساخت ماسک اغتشاش: فقط ستون‌های عددی (غیر وان‌هات) اجازه اغتشاش دارند\n",
        "    # منطق: اگر نام ستون با \"<cat>_\" شروع شود، آن ستون وان‌هاتِ دسته‌ای است.\n",
        "    numeric_mask = []\n",
        "    cat_prefixes = set(categorical_cols)\n",
        "    for name in FEATURE_NAMES:\n",
        "        is_categorical_onehot = any(name.startswith(f\"{c}_\") for c in cat_prefixes)\n",
        "        numeric_mask.append(0.0 if is_categorical_onehot else 1.0)\n",
        "    PERTURB_MASK = torch.tensor(numeric_mask, dtype=torch.float32)  # [D]\n",
        "\n",
        "    try:\n",
        "        X_train = df_train_processed[feature_cols].astype(np.float32).values\n",
        "        X_test = df_test_processed[feature_cols].astype(np.float32).values\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting feature columns to float32 before .values: {e}\")\n",
        "        for col in feature_cols:\n",
        "            try:\n",
        "                df_train_processed[col].astype(np.float32)\n",
        "            except Exception as col_e:\n",
        "                print(f\"  - Column '{col}' failed conversion: {col_e}, dtype: {df_train_processed[col].dtype}\")\n",
        "        return None, None, None, None, -1\n",
        "\n",
        "    y_train = df_train_processed['label'].astype(np.int64).values\n",
        "    y_test = df_test_processed['label'].astype(np.int64).values\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    print(f\"Preprocessing finished. Input dimension: {input_dim}\")\n",
        "    return X_train, y_train, X_test, y_test, input_dim\n",
        "\n",
        "def create_sequences(features, labels, sequence_length):\n",
        "    sequences = []\n",
        "    sequence_labels = []\n",
        "    if len(features) < sequence_length:\n",
        "        return np.array([]), np.array([])\n",
        "    for i in range(len(features) - sequence_length + 1):\n",
        "        sequences.append(features[i:i+sequence_length])\n",
        "        sequence_labels.append(labels[i+sequence_length-1])\n",
        "    return np.array(sequences, dtype=np.float32), np.array(sequence_labels, dtype=np.int64)\n",
        "\n",
        "# =========================\n",
        "# Positional Encoding\n",
        "# =========================\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# =========================\n",
        "# Transformer with Attention Output\n",
        "# =========================\n",
        "class TransformerEncoderLayerWithAttn(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, src, need_weights=False):\n",
        "        attn_out, attn_weights = self.self_attn(src, src, src, need_weights=need_weights, average_attn_weights=False)\n",
        "        src = self.norm1(src + self.dropout1(attn_out))\n",
        "        ff = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        out = self.norm2(src + self.dropout2(ff))\n",
        "        if need_weights:\n",
        "            return out, attn_weights  # [B, H, S, S]\n",
        "        return out, None\n",
        "\n",
        "class TransformerEncoderWithAttn(nn.Module):\n",
        "    def __init__(self, layer, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([layer if i == 0 else TransformerEncoderLayerWithAttn(\n",
        "            d_model=layer.self_attn.embed_dim,\n",
        "            nhead=layer.self_attn.num_heads,\n",
        "            dim_feedforward=layer.linear1.out_features,\n",
        "            dropout=layer.dropout.p\n",
        "        ) for i in range(num_layers)])\n",
        "    def forward(self, src, need_weights=False):\n",
        "        attentions = []\n",
        "        output = src\n",
        "        for layer in self.layers:\n",
        "            output, attn = layer(output, need_weights=need_weights)\n",
        "            if need_weights:\n",
        "                attentions.append(attn)\n",
        "        return output, attentions\n",
        "\n",
        "# =========================\n",
        "# Main Model: Transformer -> BiLSTM -> FC\n",
        "# =========================\n",
        "class TransformerBiLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, lstm_hidden_dim, lstm_layers, transformer_dim, nhead, num_transformer_layers, num_classes, dropout):\n",
        "        super().__init__()\n",
        "        self.transformer_dim = transformer_dim\n",
        "        self.input_proj = nn.Linear(input_dim, transformer_dim)\n",
        "        self.pos_encoder = SinusoidalPositionalEncoding(transformer_dim)\n",
        "\n",
        "        enc_layer = TransformerEncoderLayerWithAttn(\n",
        "            d_model=transformer_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=transformer_dim * 4,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.transformer_encoder = TransformerEncoderWithAttn(enc_layer, num_layers=num_transformer_layers)\n",
        "\n",
        "        self.bilstm = nn.LSTM(transformer_dim, lstm_hidden_dim,\n",
        "                              num_layers=lstm_layers, batch_first=True,\n",
        "                              dropout=dropout if lstm_layers > 1 else 0,\n",
        "                              bidirectional=True)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(lstm_hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x, return_attn=False):\n",
        "        x = self.input_proj(x) * math.sqrt(self.transformer_dim)\n",
        "        x = self.pos_encoder(x)\n",
        "        x, attn_list = self.transformer_encoder(x, need_weights=return_attn)\n",
        "        _, (h_n, _) = self.bilstm(x)\n",
        "        h_forward_last = h_n[-2]\n",
        "        h_backward_last = h_n[-1]\n",
        "        h_cat = torch.cat([h_forward_last, h_backward_last], dim=1)\n",
        "        h_cat = self.dropout_layer(h_cat)\n",
        "        logits = self.classifier(h_cat)\n",
        "        if return_attn:\n",
        "            last_attn = attn_list[-1] if len(attn_list) > 0 else None\n",
        "            return logits, last_attn\n",
        "        return logits\n",
        "\n",
        "# =========================\n",
        "# Evaluation\n",
        "# =========================\n",
        "def evaluate_model(model, data_loader, criterion, device, is_test_set=False):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    if not data_loader or len(data_loader.dataset) == 0:\n",
        "        if not is_test_set:\n",
        "            return np.nan, np.nan, np.nan\n",
        "        print(\"Warning: Cannot evaluate on empty or invalid dataloader.\")\n",
        "        return 0.0, 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in data_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            total_loss += loss.item() * batch_data.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_predictions += (predicted == batch_labels).sum().item()\n",
        "            all_labels.extend(batch_labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = correct_predictions / len(data_loader.dataset)\n",
        "    f1 = 0.0\n",
        "    if len(all_labels) > 0 and len(all_predictions) > 0:\n",
        "        f1 = f1_score(all_labels, all_predictions, average='binary', zero_division=0)\n",
        "    if is_test_set:\n",
        "        print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "        print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Test F1 Score (Binary): {f1:.4f}\")\n",
        "        if len(all_labels) > 0 and len(all_predictions) > 0:\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(confusion_matrix(all_labels, all_predictions))\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(all_labels, all_predictions, target_names=['Normal (0)', 'Attack (1)'], zero_division=0))\n",
        "        else:\n",
        "            print(\"\\nNo labels/predictions available for Confusion Matrix/Classification Report.\")\n",
        "        print(\"--------------------------------\")\n",
        "    return avg_loss, accuracy, f1\n",
        "\n",
        "# =========================\n",
        "# Adversarial Attacks (FGSM/PGD) — keep TRAIN mode, disable Dropout; mask categorical one-hot dims\n",
        "# =========================\n",
        "@contextmanager\n",
        "def train_no_dropout(model: nn.Module):\n",
        "    \"\"\"Keep model in train mode for cuDNN LSTM backward, but disable ALL Dropout modules and set LSTM.dropout=0.\"\"\"\n",
        "    was_training = model.training\n",
        "    dropout_states = []\n",
        "    lstm_dropouts = []\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout):\n",
        "            dropout_states.append((m, m.training))\n",
        "        if isinstance(m, nn.LSTM):\n",
        "            lstm_dropouts.append((m, m.dropout))\n",
        "    try:\n",
        "        model.train()\n",
        "        for m, _ in dropout_states:\n",
        "            m.train(False)\n",
        "        for lstm, d in lstm_dropouts:\n",
        "            lstm.dropout = 0.0\n",
        "        yield\n",
        "    finally:\n",
        "        if not was_training:\n",
        "            model.eval()\n",
        "        for m, was_tr in dropout_states:\n",
        "            m.train(was_tr)\n",
        "        for lstm, d in lstm_dropouts:\n",
        "            lstm.dropout = d\n",
        "\n",
        "def _mask_grad(grad):\n",
        "    if PERTURB_MASK is None:  # fallback\n",
        "        return grad\n",
        "    mask = PERTURB_MASK.to(grad.device).view(1, 1, -1)  # [1,1,D]\n",
        "    return grad * mask\n",
        "\n",
        "def _mask_step(step):\n",
        "    if PERTURB_MASK is None:\n",
        "        return step\n",
        "    mask = PERTURB_MASK.to(step.device).view(1, 1, -1)\n",
        "    return step * mask\n",
        "\n",
        "def fgsm_attack(model, x, y, loss_fn, eps, clamp=(0.0, 1.0)):\n",
        "    with train_no_dropout(model):\n",
        "        x_adv = x.detach().clone().requires_grad_(True)\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        logits = model(x_adv)\n",
        "        loss = loss_fn(logits, y)\n",
        "        grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
        "        grad = _mask_grad(grad)\n",
        "        x_adv = x_adv + eps * grad.sign()\n",
        "        x_adv = torch.clamp(x_adv, clamp[0], clamp[1]).detach()\n",
        "    return x_adv\n",
        "\n",
        "def pgd_attack(model, x, y, loss_fn, eps, alpha, steps, clamp=(0.0, 1.0)):\n",
        "    with train_no_dropout(model):\n",
        "        x_orig = x.detach()\n",
        "        x_adv = x_orig + torch.empty_like(x_orig).uniform_(-eps, eps)\n",
        "        x_adv = torch.clamp(x_adv, clamp[0], clamp[1]).detach()\n",
        "        for _ in range(steps):\n",
        "            x_adv = x_adv.clone().detach().requires_grad_(True)\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            logits = model(x_adv)\n",
        "            loss = loss_fn(logits, y)\n",
        "            grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
        "            grad = _mask_grad(grad)\n",
        "            x_adv = x_adv + _mask_step(alpha * grad.sign())\n",
        "            eta = torch.clamp(x_adv - x_orig, min=-eps, max=eps)\n",
        "            x_adv = torch.clamp(x_orig + eta, clamp[0], clamp[1]).detach()\n",
        "    return x_adv\n",
        "\n",
        "# =========================\n",
        "# Training (with optional adversarial) + Checkpointing/Resume\n",
        "# =========================\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience):\n",
        "    # سعی در ازسرگیری\n",
        "    start_epoch, best_val_loss, epochs_no_improve, history = try_load_checkpoint(model, optimizer)\n",
        "\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            if ADV_TRAINING:\n",
        "                if ADV_METHOD.lower() == 'fgsm':\n",
        "                    x_adv = fgsm_attack(model, batch_data, batch_labels, criterion, eps=ADV_EPS, clamp=(0.0, 1.0))\n",
        "                else:\n",
        "                    x_adv = pgd_attack(model, batch_data, batch_labels, criterion, eps=ADV_EPS, alpha=ADV_ALPHA, steps=ADV_STEPS, clamp=(0.0, 1.0))\n",
        "\n",
        "                logits_clean = model(batch_data)\n",
        "                logits_adv = model(x_adv)\n",
        "                loss = (1.0 - ADV_LAMBDA) * criterion(logits_clean, batch_labels) + ADV_LAMBDA * criterion(logits_adv, batch_labels)\n",
        "            else:\n",
        "                logits_clean = model(batch_data)\n",
        "                loss = criterion(logits_clean, batch_labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * batch_data.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        if val_loader:\n",
        "            val_loss, val_accuracy, val_f1_score = evaluate_model(model, val_loader, criterion, device)\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_accuracy'].append(val_accuracy)\n",
        "            history['val_f1_score'].append(val_f1_score)\n",
        "            epoch_duration = time.time() - epoch_start_time\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1_score:.4f} | Time: {epoch_duration:.2f}s\")\n",
        "\n",
        "            improved = val_loss < best_val_loss\n",
        "            if improved:\n",
        "                best_val_loss = val_loss\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            # ذخیره چک‌پوینت (latest + best)\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': model.state_dict(),\n",
        "                'optimizer_state': optimizer.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'epochs_no_improve': epochs_no_improve,\n",
        "                'history': history,\n",
        "                'hparams': {\n",
        "                    'LEARNING_RATE': LEARNING_RATE,\n",
        "                    'BATCH_SIZE': BATCH_SIZE,\n",
        "                    'ADV_TRAINING': ADV_TRAINING,\n",
        "                    'ADV_METHOD': ADV_METHOD,\n",
        "                    'ADV_EPS': ADV_EPS,\n",
        "                    'ADV_ALPHA': ADV_ALPHA,\n",
        "                    'ADV_STEPS': ADV_STEPS,\n",
        "                    'ADV_LAMBDA': ADV_LAMBDA,\n",
        "                }\n",
        "            }\n",
        "            save_checkpoint(state, is_best=improved)\n",
        "\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered after epoch {epoch+1}!\")\n",
        "                break\n",
        "\n",
        "        else:\n",
        "            # No validation\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['val_loss'].append(np.nan)\n",
        "            history['val_accuracy'].append(np.nan)\n",
        "            history['val_f1_score'].append(np.nan)\n",
        "            epoch_duration = time.time() - epoch_start_time\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Time: {epoch_duration:.2f}s\")\n",
        "\n",
        "            # ذخیره چک‌پوینت حداقلی\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': model.state_dict(),\n",
        "                'optimizer_state': optimizer.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'epochs_no_improve': epochs_no_improve,\n",
        "                'history': history\n",
        "            }\n",
        "            save_checkpoint(state, is_best=False)\n",
        "\n",
        "    total_training_time = time.time() - total_start_time\n",
        "    print(f\"--- Training Finished --- Total time: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
        "\n",
        "    # Load best (if exists)\n",
        "    if val_loader and os.path.exists(CKPT_BEST):\n",
        "        try:\n",
        "            ckpt = torch.load(CKPT_BEST, map_location=device)\n",
        "            model.load_state_dict(ckpt['model_state'])\n",
        "            print(f\"Loaded best model weights from '{CKPT_BEST}' for final evaluation.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load best model weights ({e}). Using last epoch model.\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# =========================\n",
        "# Integrated Gradients (IG)\n",
        "# =========================\n",
        "def integrated_gradients(model, x, y, steps=50, baseline='zeros', clamp=(0.0, 1.0)):\n",
        "    \"\"\"\n",
        "    x: [B, T, D], y: [B]\n",
        "    returns attributions with same shape as x\n",
        "    \"\"\"\n",
        "    x = x.detach()\n",
        "    if baseline == 'zeros':\n",
        "        x0 = torch.zeros_like(x)\n",
        "    elif baseline == 'random':\n",
        "        x0 = torch.rand_like(x)  # already in [0,1]\n",
        "    else:\n",
        "        x0 = torch.zeros_like(x)\n",
        "\n",
        "    scaled_inputs = [(x0 + (float(i) / steps) * (x - x0)).detach().requires_grad_(True) for i in range(1, steps + 1)]\n",
        "    total_grad = torch.zeros_like(x)\n",
        "\n",
        "    with train_no_dropout(model):\n",
        "        for xin in scaled_inputs:\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            logits = model(xin)\n",
        "            target_logits = logits.gather(1, y.view(-1, 1)).sum()\n",
        "            grads = torch.autograd.grad(target_logits, xin, retain_graph=False, create_graph=False)[0]\n",
        "            total_grad += grads.detach()\n",
        "\n",
        "    avg_grad = total_grad / steps\n",
        "    attributions = (x - x0) * avg_grad\n",
        "    return attributions\n",
        "\n",
        "def summarize_attributions(attr: torch.Tensor):\n",
        "    \"\"\"\n",
        "    attr: [B, T, D]\n",
        "    Returns:\n",
        "      time_importance: [T]\n",
        "      feature_importance: [D]\n",
        "    \"\"\"\n",
        "    abs_attr = attr.abs()\n",
        "    time_importance = abs_attr.sum(dim=2).mean(dim=0)      # over features then mean over batch\n",
        "    feature_importance = abs_attr.sum(dim=1).mean(dim=0)   # over time then mean over batch\n",
        "    return time_importance, feature_importance\n",
        "\n",
        "# =========================\n",
        "# Robust Evaluation (optional) — FIXED\n",
        "# =========================\n",
        "def robust_eval(model, loader, criterion, eps=0.03, steps=7, alpha=0.007, method='pgd'):\n",
        "    \"\"\"\n",
        "    Clean & adversarial accuracy. Uses no_grad() for inference and enable_grad() for attack generation.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tot, cor_clean, cor_adv = 0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "\n",
        "        # Clean accuracy (بدون گرادیان)\n",
        "        with torch.no_grad():\n",
        "            pred = model(xb).argmax(1)\n",
        "            cor_clean += (pred == yb).sum().item()\n",
        "\n",
        "        # ساخت نمونه خصمانه (با گرادیان)\n",
        "        with torch.enable_grad():\n",
        "            if method == 'fgsm':\n",
        "                xa = fgsm_attack(model, xb, yb, criterion, eps, (0.0, 1.0))\n",
        "            else:\n",
        "                xa = pgd_attack(model, xb, yb, criterion, eps, alpha, steps, (0.0, 1.0))\n",
        "\n",
        "        # دقت روی نمونه‌ی خصمانه (بدون گرادیان)\n",
        "        with torch.no_grad():\n",
        "            pred_a = model(xa).argmax(1)\n",
        "            cor_adv += (pred_a == yb).sum().item()\n",
        "\n",
        "        tot += xb.size(0)\n",
        "    print(f\"[RobustEval] Clean Acc: {cor_clean/tot:.4f} | Adv Acc ({method}, eps={eps}): {cor_adv/tot:.4f}\")\n",
        "\n",
        "# =========================\n",
        "# Pipeline\n",
        "# =========================\n",
        "print(\"Downloading UNSW-NB15 dataset using kagglehub...\")\n",
        "try:\n",
        "    dataset_path = kagglehub.dataset_download(\"dhoogla/unswnb15\")\n",
        "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Please ensure Kaggle API credentials are set up correctly in your environment.\")\n",
        "    dataset_path = None\n",
        "\n",
        "if dataset_path and os.path.isdir(dataset_path):\n",
        "    X_train_raw, y_train_raw, X_test_raw, y_test_raw, input_dim = load_and_preprocess_unsw(dataset_path)\n",
        "\n",
        "    if X_train_raw is not None and input_dim != -1:\n",
        "        INPUT_DIM = input_dim\n",
        "        print(\"Creating sequences...\")\n",
        "        X_train_seq, y_train_seq = create_sequences(X_train_raw, y_train_raw, SEQUENCE_LENGTH)\n",
        "        X_test_seq, y_test_seq = create_sequences(X_test_raw, y_test_raw, SEQUENCE_LENGTH)\n",
        "\n",
        "        if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
        "            print(\"Error: Not enough data to create sequences with the specified length.\")\n",
        "        else:\n",
        "            print(f\"Raw training sequences: {len(X_train_seq)}\")\n",
        "            print(f\"Raw testing sequences: {len(X_test_seq)}\")\n",
        "\n",
        "            # Train/Val split\n",
        "            if len(X_train_seq) > 1 and VALIDATION_SPLIT_FROM_TRAIN > 0:\n",
        "                try:\n",
        "                    X_train_final_seq, X_val_seq, y_train_final_seq, y_val_seq = train_test_split(\n",
        "                        X_train_seq, y_train_seq,\n",
        "                        test_size=VALIDATION_SPLIT_FROM_TRAIN,\n",
        "                        random_state=RANDOM_SEED, stratify=y_train_seq\n",
        "                    )\n",
        "                    print(f\"Splitting {VALIDATION_SPLIT_FROM_TRAIN*100:.1f}% of training sequences for validation.\")\n",
        "                except ValueError as e:\n",
        "                    print(f\"Warning: Could not stratify split ({e}). Performing non-stratified split.\")\n",
        "                    X_train_final_seq, X_val_seq, y_train_final_seq, y_val_seq = train_test_split(\n",
        "                        X_train_seq, y_train_seq,\n",
        "                        test_size=VALIDATION_SPLIT_FROM_TRAIN,\n",
        "                        random_state=RANDOM_SEED\n",
        "                    )\n",
        "                print(f\"Final Train sequences: {len(X_train_final_seq)}\")\n",
        "                print(f\"Validation sequences: {len(X_val_seq)}\")\n",
        "                print(f\"Final Test sequences: {len(X_test_seq)}\")\n",
        "            else:\n",
        "                print(\"Using all training sequences for training, no validation split performed.\")\n",
        "                X_train_final_seq, y_train_final_seq = X_train_seq, y_train_seq\n",
        "                X_val_seq, y_val_seq = np.array([]), np.array([])\n",
        "                print(f\"Final Train sequences: {len(X_train_final_seq)}\")\n",
        "                print(\"Validation sequences: 0\")\n",
        "                print(f\"Final Test sequences: {len(X_test_seq)}\")\n",
        "\n",
        "            if len(X_train_final_seq) > 0:\n",
        "                X_train_tensor = torch.from_numpy(X_train_final_seq)\n",
        "                y_train_tensor = torch.from_numpy(y_train_final_seq)\n",
        "                X_val_tensor = torch.from_numpy(X_val_seq)\n",
        "                y_val_tensor = torch.from_numpy(y_val_seq)\n",
        "                X_test_tensor = torch.from_numpy(X_test_seq)\n",
        "                y_test_tensor = torch.from_numpy(y_test_seq)\n",
        "\n",
        "                num_workers = 2 if DEVICE.type == 'cuda' else 0\n",
        "                pin_memory_flag = True if DEVICE.type == 'cuda' else False\n",
        "\n",
        "                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "                val_dataset = TensorDataset(X_val_tensor, y_val_tensor) if len(X_val_seq) > 0 else None\n",
        "                test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "                train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory_flag, drop_last=True)\n",
        "                val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory_flag) if val_dataset else None\n",
        "                test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n",
        "\n",
        "                model = TransformerBiLSTMModel(INPUT_DIM, LSTM_HIDDEN_DIM, LSTM_LAYERS,\n",
        "                                               TRANSFORMER_DIM, NHEAD, NUM_TRANSFORMER_LAYERS,\n",
        "                                               NUM_CLASSES, DROPOUT).to(DEVICE)\n",
        "\n",
        "                # Class weights (Normalized: N / (K * count_c))\n",
        "                if len(y_train_final_seq) > 0:\n",
        "                    counts = np.bincount(y_train_final_seq)\n",
        "                    N, K = counts.sum(), len(counts)\n",
        "                    if len(counts) == NUM_CLASSES and 0 not in counts:\n",
        "                        weights = torch.tensor([N / (K * c) for c in counts], dtype=torch.float32).to(DEVICE)\n",
        "                        criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "                        print(f\"Using normalized class weights for CrossEntropyLoss: {weights.cpu().numpy()}\")\n",
        "                    else:\n",
        "                        criterion = nn.CrossEntropyLoss()\n",
        "                else:\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "                # Train (+ Auto-Resume)\n",
        "                history = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, DEVICE, EARLY_STOPPING_PATIENCE)\n",
        "\n",
        "                # Final clean test evaluation\n",
        "                evaluate_model(model, test_loader, criterion, DEVICE, is_test_set=True)\n",
        "\n",
        "                # Optional robust evaluation\n",
        "                robust_eval(model, test_loader, criterion, eps=0.01, steps=7, alpha=0.003, method='pgd')\n",
        "                robust_eval(model, test_loader, criterion, eps=0.03, steps=7, alpha=0.007, method='pgd')\n",
        "\n",
        "                # =========================\n",
        "                # Interpretability Demo (small batch)\n",
        "                # =========================\n",
        "                if SAVE_INTERPRETABILITY:\n",
        "                    model.eval()  # برای گرفتن attention پایدار\n",
        "\n",
        "                    # نمونه کوچک از تست\n",
        "                    Xb, yb = next(iter(test_loader))\n",
        "                    Xb = Xb.to(DEVICE)\n",
        "                    yb = yb.to(DEVICE)\n",
        "\n",
        "                    # 1) Attention Maps از آخرین لایه ترنسفورمر\n",
        "                    with torch.no_grad():\n",
        "                        logits, attn = model(Xb[:1], return_attn=True)  # فقط یک نمونه برای نمایش\n",
        "                    if attn is not None:\n",
        "                        np.save(os.path.join(CKPT_DIR, 'attn_weights.npy'), attn[0].cpu().numpy())\n",
        "                        print(f\"Saved attention weights to {os.path.join(CKPT_DIR, 'attn_weights.npy')} (shape: H x T x T).\")\n",
        "\n",
        "                    # 2) Integrated Gradients برای N نمونه\n",
        "                    N_IG = min(32, Xb.size(0))\n",
        "                    attr = integrated_gradients(model, Xb[:N_IG], yb[:N_IG], steps=IG_STEPS, baseline=IG_BASELINE, clamp=(0.0, 1.0))\n",
        "                    np.save(os.path.join(CKPT_DIR, 'ig_attributions.npy'), attr.detach().cpu().numpy())\n",
        "                    print(f\"Saved Integrated Gradients attributions to {os.path.join(CKPT_DIR, 'ig_attributions.npy')} (shape: B x T x D).\")\n",
        "\n",
        "                    # Summaries\n",
        "                    t_imp, f_imp = summarize_attributions(attr.detach())\n",
        "                    t_imp_np = t_imp.cpu().numpy()\n",
        "                    f_imp_np = f_imp.cpu().numpy()\n",
        "                    np.save(os.path.join(CKPT_DIR, 'ig_time_importance.npy'), t_imp_np)\n",
        "                    np.save(os.path.join(CKPT_DIR, 'ig_feature_importance.npy'), f_imp_np)\n",
        "                    print(\"Saved IG time and feature importance.\")\n",
        "\n",
        "                    # CSV از اهمیت ویژگی‌ها با نام‌ها\n",
        "                    if FEATURE_NAMES is not None and len(FEATURE_NAMES) == f_imp_np.shape[0]:\n",
        "                        df_feat = pd.DataFrame({\n",
        "                            'feature': FEATURE_NAMES,\n",
        "                            'ig_importance': f_imp_np\n",
        "                        }).sort_values('ig_importance', ascending=False)\n",
        "                        df_feat.to_csv(os.path.join(CKPT_DIR, 'ig_feature_importance.csv'), index=False)\n",
        "                        print(f\"Saved IG feature importance CSV to {os.path.join(CKPT_DIR, 'ig_feature_importance.csv')}.\")\n",
        "\n",
        "                print(\"\\nExecution finished successfully.\")\n",
        "            else:\n",
        "                print(\"\\nExecution aborted: No training data available after sequencing/splitting.\")\n",
        "    else:\n",
        "        print(\"\\nExecution aborted due to data loading/preprocessing errors.\")\n",
        "else:\n",
        "    print(\"\\nExecution aborted: Dataset download failed or directory not found.\")\n"
      ]
    }
  ]
}